# Unsupervised Real-world Image Super Resolution via Domain-distance Aware Training

##### 前作

https://github.com/ManuelFritsche/real-world-sr/tree/master/dsgan
 是DASR的前作





### 内容与背景介绍

CVPR 2021的一篇文章

[DASR_code](https://github.com/ShuhangGu/DASR)

主要就是做unsupervised超分，通过一个unpaired HR-LR对(即图片内容不匹配，比如高清的大楼照片和低清的人脸)来训练模型对LR进行超分

[b站视频讲解](https://www.bilibili.com/video/BV1PT4y1d7b4/)

采用bibucub和bilinear等方法进行下采样再通过深度学习网络进行重建时，往往会出现严重的伪影

![image-20211125101453870](C:/Users/46752/OneDrive/mark/picture/image-20211125101453870.png)

**因为Synthetic LR难以对真实场景进行模拟，与Real-word LR有差距（domain gap 他俩就不是一类图片），所以学习Synthetic到GT HR之间的映射，应用到真实场景中就会效果不好，泛化性能有限**

在数据集的构造上就存在以下两个问题：

- 有了real-world LR的图片很难找到对应的HR
- 有real-world HR去构造LR的时候一些下采样的方法又难以模拟real-world LR

所以有了blind SR这个领域，给定一组无对应关系的real-world LR和real-world HR，让超分网络能够学习到他与与之对应的HR的对应关系

**一般的盲超分或者是real-world SR或者是unsupervsed SR,目的就是将real-world HR下采样为尽可能逼近真实图片的real-world LR，然后就构造出了配对的LR-HR对，这样训练出来的神经网络模型就能从真实的LR超分至真实的HR**

![image-20211125102857370](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/202206021052359.png)

那么进行degrade或者说downsample的方法有：

![image-20211125103314482](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/202206021052366.png)

- 调整相机的焦距构造LR-HR pair(费时费力)
- 通过先验知识 Blur Kernels和一系列的步骤去模拟下采样的过程(泛化性不强)
- 通过神经网络去学习HR和LR之间的关系，比如CycleGan(生成数据与real-world的domain gap依然存在)



### 原理简介

整体的框架如下所示

![image-20220602103017117](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/202206021030245.png)



分为两个模型



##### 1.DSN（下采样模型）

![image-20220602103604847](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/202206021036942.png)

目的是通过模型将HR下采样至LR,并且尽可能的使得生成的LR去贴近real LR，文中HR为xi而y^r是real LR，y^g是生成出的LR，y^b是Bicubic下采样的LR

通过L_con(L1 Loss)和L_per(视觉loss)保证纹理，来保证内容上的一致，使用Ladv和判别器来减小domain gap



在判别器方面：

不匹配内容的训练，容易产生伪影，所以只输入高频细节到判别器中，所以只将高频部分输入到判别器中进行判别

![image-20220602103655901](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/202206021036987.png)

GBFS uses the residual between original and Gaussian blurred images to extract high-frequency component, our WFS approach adopts Wavelet transform to obtain high-frequency component, RGB means we introduce GAN loss directly on RGB images. 

在细节上通过GBFS(Gaussian Blur Frequency Separation)来提取出高频信息，然后使用小波变换来获取高频特征，最后使用GAN loss去学习他





![image-20220602104046826](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/202206021040904.png)



判别器最终会对每一个patch输出来一个置信度，来判别生成的LR是否符合real SR的分布



##### 2.SRN(超分网络)

是由监督部分和半监督部分分别形成 Loss然后加权组合

![image-20220602104853109](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/202206021048279.png)



1.监督训练，用上一步模型生成的LR图片和真实的HR图片做一个超分，并且乘上domain distance score作为权重

2.半监督训练，用真实的LR图像和HR图像进行非配对学习，因为内容上不匹配所以只提取高频部分构造对抗损失

两个部分进行加权构成总的loss去训练，这样 **利用不同的数据特点构造不同的loss就利用到了不同的数据**



### 实验设置

有两个人为构造的复杂下采样的数据集

realSR dataset和Camera SR dataset

还有DIV2K的高清图像

有一个paired HR-LR对，是RealSR dataset,用来验证图片的效果，但是具体实验使用的unpaired HR-LR图片对，RealSR的LR和DIV2K的HR



### 总结和想法

最巧妙的是利用了图像的高频特征和L_adv去学习分布而用L1 loss去保持内容的一致性，将风格转换类GAN的想法迁移过来做一个超分，但是又不是直接用cycleGAN



判别器是一个patch D，这里就不能用U-net之类的去做一个逐像素的判别，因为不能拿判断图片的像素是不是符合real SR的分布吧，有点不合理

**但是是否可以将这种权重的形式使用attention机制去作用在训练过程中呢？**



增加可信度：添加更多的例子在补充文件中，并且开源代码有利于别人复现



JeeHyung lee



## 代码

##### 准备数据集

`source domain` indicates the **noisy and low-resolution** and `target domain` indicates the clean and **high resolution images**



##### DSN训练指令

```
python train.py --dataset hcp --artifacts tdsr \
                --generator DeResnet --discriminator FSD \
                --norm_layer Instance \
                --filter wavelet --cat_or_sum cat  \
                --batch_size 8 --num_workers 8 --crop_size 256 \
                --save_path 0603_DeResnet+wavcat+FSD+wtex0.03 \
                --upscale_factor 2
```



##### DSN测试指令

```python
python create_dataset_modified.py --dataset aim2019 \
                                  --checkpoint /home/yuzun/project/MRI_multiModel_SR/DASR/DSN_experiments/002_ori_AIM19/checkpoints/last_iteration.tar\
                                  --generator DeResnet --discriminator FSD  --filter wavelet --cat_or_sum cat\
                                  --name 002_ori_AIM19
```



`python create_dataset_modified.py --dataset aim2019 --checkpoint /home/yuzun/project/MRI_multiModel_SR/DASR/DSN_experiments/ --generator DeResnet --discriminator FSD  --filter wavelet --cat_or_sum cat --name 002_ori_AIM19`

使用源代码程序





##### 文章中使用的数据集

数据集地址： https://competitions.codalab.org/competitions/20164



##### perceptual loss

```
('--per_type', default='LPIPS', type=str, help='selecting different Perceptual loss type')
```

居然是LPIPS，这样就能提高在这个指标上的分数是吧！



## bug

`No module named 'pytorch_wavelets'`

在官网安装

https://github.com/fbcotter/pytorch_wavelets



ImportError: dlopen: cannot load any more object with static TLS It seems that scikit-image has not been built correctly.

在BasicSR的环境下有这个问题，自己安装了pytorch1.0的环境就没问题了



```
  File "/home/yuzun/project/MRI_multiModel_SR/DASR/codes/DSN/data_loader.py", line 51, in __getitem__
    resized_image = utils.imresize(clean_image, 1.0 / self.upscale_factor, True)
  File "/home/yuzun/project/MRI_multiModel_SR/DASR/codes/DSN/utils.py", line 163, in imresize
    out_2[1, :, i] = out_1_aug[1, :, idx:idx + kernel_width].mv(weights_W[i])
IndexError: index 1 is out of bounds for dimension 0 with size 1

```

dinension 0是说维度为0，也就是out_2[1]这里超出限制了，想了一下因为自然数据集是3通道而我的数据集只有1通道，所以出错，将后面两个out_2注释即可

再将DSN文件夹中的model.py 29行和46行的输入输出通道由3改为1即可

还有174行

```
class DiscriminatorBasic(nn.Module):
    def __init__(self, n_input_channels=1, norm_layer='Batch'):  # 由3改为1
```



```
Traceback (most recent call last):
  File "train.py", line 226, in <module>
    fake_img = model_g(input_img)
  File "/home/yuzun/.conda/envs/DASR_pytorch1.1.0/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/yuzun/project/MRI_multiModel_SR/DASR/codes/DSN/model.py", line 49, in forward
    block = self.block_input(x)
  File "/home/yuzun/.conda/envs/DASR_pytorch1.1.0/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/yuzun/.conda/envs/DASR_pytorch1.1.0/lib/python3.6/site-packages/torch/nn/modules/container.py", line 92, in forward
    input = module(input)
  File "/home/yuzun/.conda/envs/DASR_pytorch1.1.0/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/yuzun/.conda/envs/DASR_pytorch1.1.0/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 338, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
```

##### 环境问题

有可能是版本不兼容，比如说cuda10.0的pytorch与电脑上装的cuda11.0的不兼容

换成pytorch1.7 cuda 11.0就可以了



ImportError: dlopen: cannot load any more object with static TLS It seems that scikit-image has not been built correctly.

这个是因为pythoon版本不对，将python从3.6升级到3.7解决



`[Could not import PILLOW_VERSION from PIL](https://stackoverflow.com/questions/59659146/could-not-import-pillow-version-from-pil)`

https://stackoverflow.com/questions/59659146/could-not-import-pillow-version-from-pil

就是pillow的版本变了，在高版本pillow里没有这个'PILLOW_VERSION'了，但是torchversion的源码里还有，要不将torchvision升级要不换成 __version__



##### Loss为无穷

分析loss是怎么构造的，哪个loss最大，进行修改

1. 降低初始学习率



​	2.是tensorbord的问题可以不管，对哦这个程序里专门写了个判断是否为inf的程序说明作者在训练时应该也遇到过这个问题



3.跟perceptual loss有没有关系



loss在DSN文件夹的l

###### 尝试使用官方的数据集跑一下

数据集地址： https://competitions.codalab.org/competitions/20164



![image-20220623203936600](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/image-20220623203936600.png)

只有这两个的loss不是naf，因为step只有600的原因？

但是用这个测试出来的图片还是全黑的，是因为输出的图片不对？还是训练的有问题，数据集太大了？



##### 加载模型会卡住

每次跑程序会卡在

`self.net.load_state_dict(torch.load(model_path, **kw), strict=False)`

这个地方。换成GPU了也没用

而且是卡在第一步 torch.load这里

听说是3090与pytorch版本不兼容，咱也不懂，不折腾了

https://discuss.pytorch.org/t/training-on-rtx3090-tensor-to-device-very-slow-on-some-networks/112812

这里说明可能是版本的问题，下载的pytorch版本的cuda与cudnn不兼容，导致缺少了个东西，或者是cuda10.0在3090上就没法运行，尝试一下

我的版本是

`pytorch 1.1.0+cuda 10.0`

`python3.7.12`

确实在训练的时候GPU利用率为0



##### 加载官方模型出错

运行命令

`python create_dataset_modified.py --dataset aim2019 --checkpoint /home/yuzun/project/MRI_multiModel_SR/DASR/DSN_experiments/AIM_DeResnet_FSD.tar --generator DeResnet --discriminator FSD  --filter wavelet --cat_or_sum cat --name 002_ori_AIM19`



/home/yuzun/project/MRI_multiModel_SR/DASR/DSN_experiments/0603_DeResnet+wavcat+FSD+wtex0.03/checkpoints



```
Traceback (most recent call last):
  File "create_dataset_modified.py", line 135, in <module>
    model_d.load_state_dict(checkpoint['models_d_state_dict'])
  File "/home/yuzun/.conda/envs/DASR_pytorch1.7.0/lib/python3.7/site-packages/torch/nn/modules/module.py", line 777, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for Discriminator:
        Missing key(s) in state_dict: "DWT2.h0_col", "DWT2.h1_col", "DWT2.h0_row", "DWT2.h1_row". 
        Unexpected key(s) in state_dict: "net.net.3.weight", "net.net.3.bias", "net.net.3.running_mean", "net.net.3.running_var", "net.net.3.num_batches_tracked", "net.net.6.weight", "net.net.6.bias", "net.net.6.running_mean", "net.net.6.running_var", "net.net.6.num_batches_tracked". 
        size mismatch for net.net.0.weight: copying a param with shape torch.Size([64, 3, 5, 5]) from checkpoint, the shape in current model is torch.Size([64, 9, 5, 5]).
```

加载D的时候不对，FSD, nld_s1, nld_s2，可能有这几种选项(但是model上写了是FSD的判别器呀)

或者是选择的model不对，试了其他官方模型和D的类型**都不对**，



##### NameError: name 'target_files' is not defined

在`create_dataset_modified.py`中修改

```
elif opt.dataset == 'hcp':   # 指定test时的路径
    input_source_dir = '/data/yuzun/MRI_data/reged_ADNI/ADNI_unreged_T1img_x/A/train'
    input_target_dir = '/data/yuzun/MRI_data/hcp_all/A/train'
    source_files = [os.path.join(input_source_dir, x) for x in os.listdir(input_source_dir) if utils.is_image_file(x)]
    target_files = [os.path.join(input_target_dir, x) for x in os.listdir(input_target_dir) if utils.is_image_file(x)]
```



## 训练LPIPS作为参考指标

https://github.com/richzhang/PerceptualSimilarity

可以用自己的数据集训练



### 高清MRI

https://github.com/linhandev/dataset





以gan为baseline的会比较lpips这个指标

是不是可以用其他不用视觉的传统模型去把这个PSNR刷高





## stytr

`CUDA_VISIBLE_DEVICES=0,1 python train.py --style_dir /data/yuzun/MRI_data/ADNI1_x/A/train --content_dir /data/yuzun/MRI_data/hcp_all/flip/train --save_dir /data/yuzun/StyTR-2_model --batch_size 6`



test `CUDA_VISIBLE_DEVICES=0 python test.py --style_dir /data/yuzun/MRI_data/ADNI1_x/A/train --content_dir /data/yuzun/MRI_data/hcp_all/flip/train --output out --decoder_path /data/yuzun/StyTR-2_model/decoder_iter_160000.pth --Trans_path /data/yuzun/StyTR-2_model/transformer_iter_160000.pth --embedding_path /data/yuzun/StyTR-2_model/embedding_iter_160000.pth`

https://github.com/diyiiyiii/StyTR-2





第一次跑出来结果

![image-20220623153540329](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/image-20220623153540329.png)

这个是loss吗？是不是大了点.loss确实很大

训练出来的模型不对，loss太大了，所以出来的图片是黑的

其实style dir中的图片只要放一张就好了，看了原论文，他是丛网上爬取的艺术照片，然后每一张照片是一个风格

然后在test的时候，要`output = output[0]`，因为模型输出的是一个tuple,将loss啥的全部输出了，所以只要第一个才是真正的输出



##### style dir图片的数量

![image-20220625105354456](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/image-20220625105354456.png)





##### 第一次训练结果

命令

在82K的时候loss下降

![image-20220624085121186](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/image-20220624085121186.png)

但是test的时候还是黑色的，但是在保存模型的那个test那里又是有输出的 

查看保存模型test的函数是怎么写的

是不是模型加载有问题，是不是要用GPU加载(在train的test里面是一个batchsize输入进行预测的)



test里的图片按道理应该是 stylt,content,还有out的顺序，但是后面顺序乱了

顺序没有乱，是从左到右的顺序，因为调用的是`torchvision`里的save_image函数，当多个tensor cat在一起时会分开保存，然后就保存，但是他画的格子是8X8的，我们的batchsize是6，所以后面几个是没有值的

![image-20220624102512148](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/image-20220624102512148.png)

从左到右数6个是style,后面6个是content,再后面是out



在训练的时候有输出，在test的时候输出为全黑，但是用官方的代码又是有输出的，不知道为什么。。。。

要不就是存储模型的问题

要不就是保存模型的问题



加载模型时只要有`Trans.eval()`就会只输出黑色的图片，因为trans里面有dropout层吗？那即使不加，输出图片的效果也不好



##### test时候要把StyTR中229行的去改变



##### log会覆盖

生成的log好像会覆盖原来的log，麻了





##### 论文理论



##### 更改图片尺寸大小



##### 加上全部ADNI1







### 要看的论文

Uncertainty-aware Generalized Adaptive CycleGAN(论文.md)  从充满噪声的LR中生成其他模态



### BasicSR

##### 怀疑测试和val时不一样

根据群里xintao大佬所说

`models/sr_models` 118行将`if self.ema_decay > 0 `注释掉，尝试了没用



实验在val时直接推理ADNI1试试



28行这样改

![image-20220628201142406](https://raw.githubusercontent.com/DejaVuyan/blog.img/main/image-20220628201142406.png)

